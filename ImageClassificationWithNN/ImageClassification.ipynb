{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5333aa0a",
   "metadata": {},
   "source": [
    "## Image Transformation Pipeline (PyTorch)\n",
    "\n",
    "Before feeding images into a neural network, we apply a sequence of transformations to ensure they are uniform, normalized, and tensorized. Hereâ€™s what each step does:\n",
    "\n",
    "| Step | Transformation | Purpose | Notes |\n",
    "|------|----------------|---------|-------|\n",
    "| 1    | `transforms.Resize((64, 64))` | Resizes all images to 64Ã—64 pixels | Neural networks require fixed-size inputs |\n",
    "| 2    | `transforms.ToTensor()` | Converts image to PyTorch tensor (and scales pixel values from [0, 255] to [0, 1]) | Changes shape from (H, W, C) to (C, H, W) |\n",
    "| 3    | `transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])` | Normalizes R, G, B channels to have values in [âˆ’1, +1] | Helps model converge faster with zero-centered inputs |\n",
    "\n",
    "> Together, this pipeline ensures your image data is in the optimal format and scale for training a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "656eb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms:\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import random\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3,[0.5]*3)\n",
    "])\n",
    "\n",
    "#Loading the dataset:\n",
    "#Automatically assigns labels & transforms: dog folder => label 0, etc\n",
    "full_dataset = datasets.ImageFolder(root='raw-img', transform=transform)\n",
    "\n",
    "#Balancing the dataset(Since more 1's are present):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a2d7f",
   "metadata": {},
   "source": [
    "## Splitting the Dataset (Stratified Split Method)\n",
    "* Random Data split did not seem to work, so had to ask chatGPT to properly split the data for me for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e60c81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group indices by class\n",
    "class_indices = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(full_dataset):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# Choose exactly 1446 samples from each class\n",
    "target_count = 1446\n",
    "selected_indices = []\n",
    "\n",
    "for label, indices in class_indices.items():\n",
    "    if len(indices) >= target_count:\n",
    "        sampled = random.sample(indices, target_count)\n",
    "        selected_indices.extend(sampled)\n",
    "    else:\n",
    "        raise ValueError(f\"Not enough samples in class {label} to select {target_count}\")\n",
    "\n",
    "# Shuffle all selected indices\n",
    "random.shuffle(selected_indices)\n",
    "\n",
    "# Now, split into train/test (e.g., 80/20 split)\n",
    "split_point = int(0.8 * len(selected_indices))\n",
    "train_indices = selected_indices[:split_point]\n",
    "test_indices = selected_indices[split_point:]\n",
    "\n",
    "# Create Subsets\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1500b8",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "* Mini-Batch Training\n",
    "* Shuffling - Randomizes the data order\n",
    "* Automatic batching - Done automatically by Pytorch\n",
    "\n",
    "DataLoader is a utility that takes a dataset, splits it into mini-batches and feeds our model in chunks instead of all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c6e4d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_dataset,batch_size=64,shuffle=False)\n",
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf5192",
   "metadata": {},
   "source": [
    "## Coding the Neural Network\n",
    "* Little bit of Theory by chatGPT:\n",
    "\n",
    "##  Feedforward Neural Network Architecture\n",
    "\n",
    "We define a custom neural network using PyTorch's `nn.Module` class. This network takes a flattened 64Ã—64 RGB image (12288 features) and passes it through 3 fully connected (dense) layers with sigmoid activation functions.\n",
    "\n",
    "###  Model Architecture\n",
    "\n",
    "| Layer     | Description                            | Output Shape     |\n",
    "|-----------|----------------------------------------|------------------|\n",
    "| Flatten   | Converts 3D image (3Ã—64Ã—64) to 1D      | (12288,1)         |\n",
    "| Linear 1  | Fully connected layer (12288 â†’ 3)      | (3,1)             |\n",
    "| Sigmoid   | Activation after Layer 1               | (3,1)             |\n",
    "| Linear 2  | Fully connected layer (3 â†’ 2)          | (2,1)             |\n",
    "| Sigmoid   | Activation after Layer 2               | (2,1)             |\n",
    "| Linear 3  | Fully connected layer (2 â†’ 1)          | (1,1)             |\n",
    "| Sigmoid   | Activation after output layer          | (1,1)             |\n",
    "\n",
    "###  Explanation of Components\n",
    "\n",
    "- `nn.Flatten()`  \n",
    "  Flattens the 3D input tensor `(3, 64, 64)` to a 1D vector of size `12288`.\n",
    "\n",
    "- `nn.Linear(input_dim, output_dim)`  \n",
    "  A fully connected layer that learns weights and biases to transform data.\n",
    "\n",
    "- `nn.Sigmoid()`  \n",
    "  Activation function that squashes output to range (0, 1), suitable for binary classification.\n",
    "\n",
    "###  Initialization\n",
    "\n",
    "By default, PyTorch initializes weights using **Kaiming Uniform Initialization**, or **He Uniform Initialization**, which works well even with sigmoid in small networks.\n",
    "\n",
    "Biases are initialised by default from a uniform distribution: bias ~ U(-1/sqrt(n_in),1/sqrt(n_in))\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3f793300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "# #Our Feedforward NN:\n",
    "# class AnimalClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(AnimalClassifier, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc1 = nn.Linear(64*64*3,3) #Linear Initialisation from X to first layer(3 neurons)\n",
    "#         self.fc2 = nn.Linear(3,2)   #3 Neurons to 2 Neurons\n",
    "#         #self.fc3 = nn.Linear(2,1)   #2 Neurons to 1 Neuron if binary\n",
    "#         self.fc3 = nn.Linear(2,3)   #We are doing multi-class, not binary!!!\n",
    "#         self.sigmoid = nn.Sigmoid() #Sigmoid is our activation function\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         x = self.flatten(x) #Flattens pixels to a column vector\n",
    "#         x = self.sigmoid(self.fc1(x)) #x now becomes a1(3x1 column vector)\n",
    "#         x = self.sigmoid(self.fc2(x)) #x now becomes a2(2x1 column vector)\n",
    "#         #x = self.sigmoid(self.fc3(x)) This was a mistake!!\n",
    "#         x = self.fc3(x) #x now becomes y(hat), the prediction/final output\n",
    "#         #Hidden layers: Sigmoid or ReLU\n",
    "#         #Final output layer => When using CrossEntropyLoss, it is bad\n",
    "#         #Because PyTorch expects raw scores(logits) from the last layer\n",
    "#         #It automatically applies log(softmax) internally\n",
    "#         return x\n",
    "\n",
    "#The class above was using sigmoid, now let's use ReLU and MORE Neurons!\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AnimalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AnimalClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(3 * 64 * 64, 128)  # Input layer to hidden layer 1\n",
    "        self.fc2 = nn.Linear(128, 64)           # Hidden layer 1 to hidden layer 2\n",
    "        self.fc3 = nn.Linear(64, 3)             # Hidden layer 2 to output (3 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)     # Flatten image tensor from [B, 3, 64, 64] to [B, 12288]\n",
    "        x = F.relu(self.fc1(x))       # Apply ReLU after first hidden layer\n",
    "        x = F.relu(self.fc2(x))       # Apply ReLU after second hidden layer\n",
    "        x = self.fc3(x)               # Final layer, output raw logits\n",
    "        return x\n",
    "\n",
    "\n",
    "model = AnimalClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451287a5",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer\n",
    "\n",
    "### Binary Cross-Entropy Loss (BCELoss)\n",
    "\n",
    "Since this is a **binary classification** task, we use the **Binary Cross-Entropy Loss** to measure how well the model is predicting 0 or 1.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\n",
    "\\text{Loss} = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "\n",
    "$$\n",
    "\n",
    "- hat{y}  : predicted probability from the model\n",
    "- \\( y \\): actual label (0 or 1)\n",
    "In PyTorch:\n",
    "\n",
    "```python\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0036b51",
   "metadata": {},
   "source": [
    "### Optimizer â€” Stochastic Gradient Descent (SGD)\n",
    "\n",
    "The optimizer is responsible for updating the modelâ€™s parameters (weights and biases) to reduce the loss.\n",
    "\n",
    "We use **Stochastic Gradient Descent (SGD)**, which updates parameters using small batches of data (mini-batch gradient descent):\n",
    "\n",
    "- Each batch is passed through the model\n",
    "- Gradients are computed via backpropagation\n",
    "- Parameters are updated to minimize the loss\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2fa2cab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnimalClassifier(\n",
       "  (fc1): Linear(in_features=12288, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "#Loss function:\n",
    "criterion = nn.CrossEntropyLoss() #Not binary!!! Very important!\n",
    "#If our project was benign/malignant tumors, we use BCEloss() for our NLL\n",
    "\n",
    "\n",
    "\n",
    "#Using Stochastic Gradient Descent(SGD), define learning rate also\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) \n",
    "#One other function to move your model to the GPU(Faster Training):\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#So now, we need to train this variable model that we created in previous code\n",
    "#Training done with a concept called training loop, in next code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3aff8",
   "metadata": {},
   "source": [
    "##  What is the Training Loop in Neural Networks?\n",
    "\n",
    "The **training loop** is the core process that allows a neural network to learn from data. Itâ€™s a repeated cycle of:\n",
    "\n",
    "1. **Forward Pass**: The model makes predictions on the input data.\n",
    "2. **Loss Calculation**: We compute how wrong those predictions are using a loss function.\n",
    "3. **Backward Pass**: Gradients (slopes) of the loss w.r.t. model parameters are calculated using backpropagation.\n",
    "4. **Optimizer Step**: The optimizer updates the modelâ€™s weights and biases to reduce the loss.\n",
    "\n",
    "This loop is repeated for multiple **epochs**, where:\n",
    "- **1 epoch** = 1 full pass through the training dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Why is this necessary?\n",
    "\n",
    "- Neural networks start with random weights.\n",
    "- The only way they learn is through **gradually adjusting** those weights to minimize error.\n",
    "- Each pass through the loop helps the model make **better predictions**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Training Loop Components Summary\n",
    "\n",
    "| Step              | Description                                                  |\n",
    "|-------------------|--------------------------------------------------------------|\n",
    "| Forward Pass      | Model predicts output using current weights                  |\n",
    "| Loss Calculation  | Measures how far predictions are from true values            |\n",
    "| Backward Pass     | Computes how to change weights to reduce the loss            |\n",
    "| Optimizer Step    | Applies those changes to update the modelâ€™s parameters       |\n",
    "| Repeat for Epochs | Iterate over the entire dataset multiple times               |\n",
    "\n",
    "---\n",
    "\n",
    "> Without this loop, the neural network cannot improve â€” it would just keep guessing randomly.\n",
    "\n",
    "> No. of iterations = epochs, similar to timesteps when using ABAQUS! the neural network cannot improve â€” it would just keep guessing randomly.\n",
    "\n",
    "##  What is an Epoch in Machine Learning?\n",
    "\n",
    "An **epoch** is **one complete pass** through the **entire training dataset**.\n",
    "\n",
    "Imagine your model as a student studying a textbook (the dataset):\n",
    "- **One epoch** = the student reads the entire book once.\n",
    "- **Multiple epochs** = the student reads the book multiple times to better memorize and understand.\n",
    "\n",
    "---\n",
    "\n",
    "##  Why Do We Need Multiple Epochs?\n",
    "\n",
    "- **First epoch**: The model starts with random guesses and learns basic patterns.\n",
    "- **Subsequent epochs**: The model adjusts its internal weights and improves based on errors from the previous pass.\n",
    "- With each pass, the model *learns from its mistakes* and gets better.\n",
    "\n",
    "---\n",
    "\n",
    "##  Effect of Increasing Epochs\n",
    "\n",
    "- Initially: Training improves rapidly.\n",
    "- Later: Improvements become smaller.\n",
    "- Too many: Risk of **overfitting** â€” the model memorizes training data and fails on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "##  Epochs vs. Batches vs. Iterations\n",
    "\n",
    "| Term       | Meaning                                               |\n",
    "|------------|--------------------------------------------------------|\n",
    "| **Epoch**  | One full pass over the training data                   |\n",
    "| **Batch**  | A subset of the training data used for one update     |\n",
    "| **Iteration** | One update of weights (one batch passed)           |\n",
    "\n",
    "If you have 1000 images and use batch size 100:\n",
    "- 1 epoch = 10 iterations\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "- Increasing epochs lets the model learn **better patterns**.\n",
    "- But you need to stop when loss stops improving (use early stopping or plotting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b5239d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 57.2527\n",
      "Epoch 2/40, Loss: 52.7017\n",
      "Epoch 3/40, Loss: 49.4655\n",
      "Epoch 4/40, Loss: 47.0415\n",
      "Epoch 5/40, Loss: 45.0661\n",
      "Epoch 6/40, Loss: 43.4036\n",
      "Epoch 7/40, Loss: 41.9545\n",
      "Epoch 8/40, Loss: 40.6502\n",
      "Epoch 9/40, Loss: 39.4233\n",
      "Epoch 10/40, Loss: 38.2424\n",
      "Epoch 11/40, Loss: 37.0726\n",
      "Epoch 12/40, Loss: 35.8846\n",
      "Epoch 13/40, Loss: 34.6828\n",
      "Epoch 14/40, Loss: 33.4445\n",
      "Epoch 15/40, Loss: 32.1726\n",
      "Epoch 16/40, Loss: 30.8774\n",
      "Epoch 17/40, Loss: 29.5929\n",
      "Epoch 18/40, Loss: 28.2962\n",
      "Epoch 19/40, Loss: 26.9821\n",
      "Epoch 20/40, Loss: 25.6715\n",
      "Epoch 21/40, Loss: 24.3612\n",
      "Epoch 22/40, Loss: 23.0297\n",
      "Epoch 23/40, Loss: 21.7326\n",
      "Epoch 24/40, Loss: 20.4789\n",
      "Epoch 25/40, Loss: 19.2332\n",
      "Epoch 26/40, Loss: 17.9505\n",
      "Epoch 27/40, Loss: 16.8031\n",
      "Epoch 28/40, Loss: 15.6159\n",
      "Epoch 29/40, Loss: 14.4741\n",
      "Epoch 30/40, Loss: 13.4871\n",
      "Epoch 31/40, Loss: 15.1705\n",
      "Epoch 32/40, Loss: 13.0299\n",
      "Epoch 33/40, Loss: 11.5296\n",
      "Epoch 34/40, Loss: 12.8351\n",
      "Epoch 35/40, Loss: 9.5675\n",
      "Epoch 36/40, Loss: 8.2821\n",
      "Epoch 37/40, Loss: 8.0702\n",
      "Epoch 38/40, Loss: 7.5832\n",
      "Epoch 39/40, Loss: 6.5124\n",
      "Epoch 40/40, Loss: 5.6473\n",
      "Predictions: tensor([1, 1, 0, 1, 2, 2, 0, 2, 1, 1])\n",
      "Targets    : tensor([1, 1, 0, 1, 2, 2, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "#Training loop skeleton:\n",
    "#num_epochs = 10 #Number of times we pass through the entire dataset => Loss was around 70-80\n",
    "num_epochs = 40 #Now, let's use more epochs!\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0 #Tracks total loss for a specific epoch\n",
    "\n",
    "    #Now, in an epoch, we iterate over mini-batches of data\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device) #Moved to GPU!\n",
    "\n",
    "            #Now, dog, which is 0 needs to become 0.0, Elephant which is 2->2.0\n",
    "            #Convert to float so that we can add/subtract in BCE\n",
    "            #labels = labels.float().unsqueeze(1)\n",
    "        #But, we are doing CrossEntropyLoss!!!\n",
    "        #So, we remove that line, keep it integer valued\n",
    "\n",
    "        #Clear previous gradients for each epoch, we get a different value\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Forward pass first, using linear initialization, basically,\n",
    "        #We put train_loader, which is a random batch of images into the model\n",
    "        outputs = model(images) #So now, we have our prediction(yhat)\n",
    "        loss = criterion(outputs,labels)\n",
    "        #Now, computation of loss(BCE): Takes in the prediction(yhat) and label(y)\n",
    "        #First mistake: this is for one class:  \n",
    "        #For multi-class classification, we need to have a different loss:\n",
    "        \n",
    "        #Now, perform backpropagation, simply with 1 line, gradients calculated:\n",
    "        loss.backward() \n",
    "\n",
    "        #For our optimizer function: weights and biases to be updated\n",
    "        optimizer.step()\n",
    "\n",
    "        #Now, for this batch for this epoch, add the loss onto running loss\n",
    "        running_loss+= loss.item()\n",
    "    \n",
    "    #Now, let's see what the loss of each iteration/epoch is:\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "#Now, printing predictions and targets:\n",
    "images, labels = next(iter(train_loader))\n",
    "outputs = model(images.to(device))\n",
    "_, preds = torch.max(outputs, 1)\n",
    "\n",
    "print(\"Predictions:\", preds[:10])\n",
    "print(\"Targets    :\", labels[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4cad8",
   "metadata": {},
   "source": [
    "### Testing Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "30d6fd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "model.eval() #Evaluation mode\n",
    "\n",
    "#For collecting all predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images) #Forward Pass\n",
    "        _, predicted = torch.max(outputs.data,1) #Choose class with highest score as prediction\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "#Confusion Matrix:\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "#Plotting:\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690f4db",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "The original goal of this project was to build a basic feedforward neural network to classify animal images (`dog`, `elephant`, `butterfly`) from the Animals-10 dataset. Throughout the development process, several key modifications were made to improve model performance, stability, and training correctness. Below is a complete list of all refinements and theoretical insights added along the way:\n",
    "\n",
    "## 1. Model Architecture Enhancements\n",
    "\n",
    "| Update | Description |\n",
    "|--------|-------------|\n",
    "| **Increased Neurons** | Changed architecture from a minimal 3-2-1 structure to a deeper network with more neurons in each layer, enabling the model to learn more complex patterns. |\n",
    "| **Changed Activation to ReLU** | Replaced `Sigmoid` with `ReLU` in hidden layers for better convergence, especially in deeper networks. |\n",
    "| **Reason** | ReLU avoids the vanishing gradient problem and speeds up training compared to Sigmoid. |\n",
    "| **Final Output Layer** | For multi-class output, the final layer outputs raw logits instead of sigmoid probabilities. |\n",
    "\n",
    "## 2. Loss Function Transition\n",
    "\n",
    "### ðŸ”„ Why We Changed the Loss Function\n",
    "\n",
    "| Aspect                  | Details                                                                 |\n",
    "|-------------------------|-------------------------------------------------------------------------|\n",
    "| **Initial Choice**      | `Binary Cross Entropy (BCE)`                                            |\n",
    "| **Issue**               | BCE is only suitable for **binary classification** tasks.               |\n",
    "|                         | We are solving a **multi-class** problem (e.g., dog, butterfly, elephant). |\n",
    "| **Problem Faced**       | Using BCE led to errors due to incorrect output and target shapes.      |\n",
    "| **Final Choice**        | Switched to `CrossEntropyLoss()` â€” designed for multi-class classification. |\n",
    "| **Result**              | Model now accepts raw logits and properly computes multi-class loss.    |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® Key Differences\n",
    "\n",
    "| Loss Type                  | Formula                                                                 | Notes                                                                 |\n",
    "|---------------------------|-------------------------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| **Binary Cross Entropy**  | `- [ y * log(yÌ‚) + (1 - y) * log(1 - yÌ‚) ]`                             | Expects probabilities (`yÌ‚` in [0, 1]), typically used with `Sigmoid`.|\n",
    "| **Categorical Cross Entropy** | `-log( e^{z_k} / âˆ‘ e^{z_j} )`<br> â†’ `-log(Softmax(logits)[k])`        | Expects raw logits (no softmax needed). Best for multi-class output. |\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… `CrossEntropyLoss()` is appropriate for classification problems where each input belongs to **exactly one class out of many**.\n",
    "\n",
    "\n",
    "## 3. Training Loop and Optimizer\n",
    "\n",
    "| Element | Explanation |\n",
    "|---------|-------------|\n",
    "| **Loss Tracking** | Implemented per-epoch loss reporting to observe convergence over time. |\n",
    "| **Optimizer Used** | `Stochastic Gradient Descent (SGD)` with fixed learning rate. |\n",
    "| **Epoch Intuition** | Explained epochs as \"full passes through the dataset\", allowing the model to iteratively refine weights. |\n",
    "| **Overfitting Risk** | Discussed how increasing epochs too much can lead to overfitting on training data. |\n",
    "\n",
    "## 4. Output Interpretation Fix\n",
    "\n",
    "| Update | Explanation |\n",
    "|--------|-------------|\n",
    "| **Model Output (`yÌ‚`)** | Clarified that logits (not probabilities) should be passed to `CrossEntropyLoss`. |\n",
    "| **Prediction Strategy** | After training, `argmax` was used to convert logits into predicted labels. |\n",
    "\n",
    "## 5. Evaluation Improvements\n",
    "\n",
    "| Topic | Details |\n",
    "|-------|---------|\n",
    "| **Confusion Matrix** | Used to evaluate class-wise performance. |\n",
    "| **Accuracy Metric** | Implemented to check model's overall predictive success. |\n",
    "| **Precision/Recall Discussion** | Discussed their role in imbalanced datasets (though not yet implemented). |\n",
    "\n",
    "## 6. Dataset Management Fixes\n",
    "\n",
    "| Fix | Description |\n",
    "|-----|-------------|\n",
    "| **Unbalanced Labels** | Identified severe class imbalance (e.g., 1446 dog images vs. 300 others). |\n",
    "| **Solution** | Balanced the dataset by sampling an equal number (e.g., 1446) from each class. |\n",
    "| **Manual Verification** | Counted class-wise samples to confirm balance before training. |\n",
    "\n",
    "## Final Remarks\n",
    "\n",
    "This iterative project evolved from a basic neural net to a well-structured, multi-class classification system with correct architectural, loss function, and dataset handling strategies. Each improvement addressed a specific challenge:\n",
    "\n",
    "- Inappropriate loss function â†’ switched to `CrossEntropyLoss`\n",
    "- Vanishing gradients â†’ switched to `ReLU`\n",
    "- Class imbalance â†’ fixed with balanced sampling\n",
    "- Output mismatch â†’ corrected with `argmax` + logits\n",
    "\n",
    "> The process has strengthened both your understanding of neural networks and your practical skills in debugging and refining real-world ML systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
