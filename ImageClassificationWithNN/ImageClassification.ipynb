{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5333aa0a",
   "metadata": {},
   "source": [
    "## Image Transformation Pipeline (PyTorch)\n",
    "\n",
    "Before feeding images into a neural network, we apply a sequence of transformations to ensure they are uniform, normalized, and tensorized. Hereâ€™s what each step does:\n",
    "\n",
    "| Step | Transformation | Purpose | Notes |\n",
    "|------|----------------|---------|-------|\n",
    "| 1    | `transforms.Resize((64, 64))` | Resizes all images to 64Ã—64 pixels | Neural networks require fixed-size inputs |\n",
    "| 2    | `transforms.ToTensor()` | Converts image to PyTorch tensor (and scales pixel values from [0, 255] to [0, 1]) | Changes shape from (H, W, C) to (C, H, W) |\n",
    "| 3    | `transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])` | Normalizes R, G, B channels to have values in [âˆ’1, +1] | Helps model converge faster with zero-centered inputs |\n",
    "\n",
    "> Together, this pipeline ensures your image data is in the optimal format and scale for training a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656eb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms:\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import random\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3,[0.5]*3)\n",
    "])\n",
    "\n",
    "#Loading the dataset:\n",
    "#Automatically assigns labels & transforms: dog folder => label 0, etc\n",
    "full_dataset = datasets.ImageFolder(root='raw-img', transform=transform)\n",
    "\n",
    "#Balancing the dataset(Since more 1's are present):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a2d7f",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "* Random Data split did not seem to work, so had to ask chatGPT to properly split the data for me for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60c81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group indices by class\n",
    "class_indices = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(full_dataset):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# Choose exactly 1446 samples from each class\n",
    "target_count = 1446\n",
    "selected_indices = []\n",
    "\n",
    "for label, indices in class_indices.items():\n",
    "    if len(indices) >= target_count:\n",
    "        sampled = random.sample(indices, target_count)\n",
    "        selected_indices.extend(sampled)\n",
    "    else:\n",
    "        raise ValueError(f\"Not enough samples in class {label} to select {target_count}\")\n",
    "\n",
    "# Shuffle all selected indices\n",
    "random.shuffle(selected_indices)\n",
    "\n",
    "# Now, split into train/test (e.g., 80/20 split)\n",
    "split_point = int(0.8 * len(selected_indices))\n",
    "train_indices = selected_indices[:split_point]\n",
    "test_indices = selected_indices[split_point:]\n",
    "\n",
    "# Create Subsets\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1500b8",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "* Mini-Batch Training\n",
    "* Shuffling - Randomizes the data order\n",
    "* Automatic batching - Done automatically by Pytorch\n",
    "\n",
    "DataLoader is a utility that takes a dataset, splits it into mini-batches and feeds our model in chunks instead of all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e4d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_dataset,batch_size=64,shuffle=False)\n",
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf5192",
   "metadata": {},
   "source": [
    "## Coding the Neural Network\n",
    "* Little bit of Theory by chatGPT:\n",
    "\n",
    "##  Feedforward Neural Network Architecture\n",
    "\n",
    "We define a custom neural network using PyTorch's `nn.Module` class. This network takes a flattened 64Ã—64 RGB image (12288 features) and passes it through 3 fully connected (dense) layers with sigmoid activation functions.\n",
    "\n",
    "###  Model Architecture\n",
    "\n",
    "| Layer     | Description                            | Output Shape     |\n",
    "|-----------|----------------------------------------|------------------|\n",
    "| Flatten   | Converts 3D image (3Ã—64Ã—64) to 1D      | (12288,1)         |\n",
    "| Linear 1  | Fully connected layer (12288 â†’ 3)      | (3,1)             |\n",
    "| Sigmoid   | Activation after Layer 1               | (3,1)             |\n",
    "| Linear 2  | Fully connected layer (3 â†’ 2)          | (2,1)             |\n",
    "| Sigmoid   | Activation after Layer 2               | (2,1)             |\n",
    "| Linear 3  | Fully connected layer (2 â†’ 1)          | (1,1)             |\n",
    "| Sigmoid   | Activation after output layer          | (1,1)             |\n",
    "\n",
    "###  Explanation of Components\n",
    "\n",
    "- `nn.Flatten()`  \n",
    "  Flattens the 3D input tensor `(3, 64, 64)` to a 1D vector of size `12288`.\n",
    "\n",
    "- `nn.Linear(input_dim, output_dim)`  \n",
    "  A fully connected layer that learns weights and biases to transform data.\n",
    "\n",
    "- `nn.Sigmoid()`  \n",
    "  Activation function that squashes output to range (0, 1), suitable for binary classification.\n",
    "\n",
    "###  Initialization\n",
    "\n",
    "By default, PyTorch initializes weights using **Kaiming Uniform Initialization**, or **He Uniform Initialization**, which works well even with sigmoid in small networks.\n",
    "\n",
    "Biases are initialised by default from a uniform distribution: bias ~ U(-1/sqrt(n_in),1/sqrt(n_in))\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f793300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "# #Our Feedforward NN:\n",
    "# class AnimalClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(AnimalClassifier, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc1 = nn.Linear(64*64*3,3) #Linear Initialisation from X to first layer(3 neurons)\n",
    "#         self.fc2 = nn.Linear(3,2)   #3 Neurons to 2 Neurons\n",
    "#         #self.fc3 = nn.Linear(2,1)   #2 Neurons to 1 Neuron if binary\n",
    "#         self.fc3 = nn.Linear(2,3)   #We are doing multi-class, not binary!!!\n",
    "#         self.sigmoid = nn.Sigmoid() #Sigmoid is our activation function\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         x = self.flatten(x) #Flattens pixels to a column vector\n",
    "#         x = self.sigmoid(self.fc1(x)) #x now becomes a1(3x1 column vector)\n",
    "#         x = self.sigmoid(self.fc2(x)) #x now becomes a2(2x1 column vector)\n",
    "#         #x = self.sigmoid(self.fc3(x)) This was a mistake!!\n",
    "#         x = self.fc3(x) #x now becomes y(hat), the prediction/final output\n",
    "#         #Hidden layers: Sigmoid or ReLU\n",
    "#         #Final output layer => When using CrossEntropyLoss, it is bad\n",
    "#         #Because PyTorch expects raw scores(logits) from the last layer\n",
    "#         #It automatically applies log(softmax) internally\n",
    "#         return x\n",
    "\n",
    "#The class above was using sigmoid, now let's use ReLU and MORE Neurons!\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AnimalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AnimalClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(3 * 64 * 64, 128)  # Input layer to hidden layer 1\n",
    "        self.fc2 = nn.Linear(128, 64)           # Hidden layer 1 to hidden layer 2\n",
    "        self.fc3 = nn.Linear(64, 3)             # Hidden layer 2 to output (3 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)     # Flatten image tensor from [B, 3, 64, 64] to [B, 12288]\n",
    "        x = F.relu(self.fc1(x))       # Apply ReLU after first hidden layer\n",
    "        x = F.relu(self.fc2(x))       # Apply ReLU after second hidden layer\n",
    "        x = self.fc3(x)               # Final layer, output raw logits\n",
    "        return x\n",
    "\n",
    "\n",
    "model = AnimalClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451287a5",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer\n",
    "\n",
    "### Binary Cross-Entropy Loss (BCELoss)\n",
    "\n",
    "Since this is a **binary classification** task, we use the **Binary Cross-Entropy Loss** to measure how well the model is predicting 0 or 1.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\n",
    "\\text{Loss} = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "\n",
    "$$\n",
    "\n",
    "- hat{y}  : predicted probability from the model\n",
    "- \\( y \\): actual label (0 or 1)\n",
    "In PyTorch:\n",
    "\n",
    "```python\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0036b51",
   "metadata": {},
   "source": [
    "### Optimizer â€” Stochastic Gradient Descent (SGD)\n",
    "\n",
    "The optimizer is responsible for updating the modelâ€™s parameters (weights and biases) to reduce the loss.\n",
    "\n",
    "We use **Stochastic Gradient Descent (SGD)**, which updates parameters using small batches of data (mini-batch gradient descent):\n",
    "\n",
    "- Each batch is passed through the model\n",
    "- Gradients are computed via backpropagation\n",
    "- Parameters are updated to minimize the loss\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa2cab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnimalClassifier(\n",
       "  (fc1): Linear(in_features=12288, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "#Loss function:\n",
    "criterion = nn.CrossEntropyLoss() #Not binary!!! Very important!\n",
    "#If our project was benign/malignant tumors, we use BCEloss() for our NLL\n",
    "\n",
    "\n",
    "\n",
    "#Using Stochastic Gradient Descent(SGD), define learning rate also\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) \n",
    "#One other function to move your model to the GPU(Faster Training):\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#So now, we need to train this variable model that we created in previous code\n",
    "#Training done with a concept called training loop, in next code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3aff8",
   "metadata": {},
   "source": [
    "##  What is the Training Loop in Neural Networks?\n",
    "\n",
    "The **training loop** is the core process that allows a neural network to learn from data. Itâ€™s a repeated cycle of:\n",
    "\n",
    "1. **Forward Pass**: The model makes predictions on the input data.\n",
    "2. **Loss Calculation**: We compute how wrong those predictions are using a loss function.\n",
    "3. **Backward Pass**: Gradients (slopes) of the loss w.r.t. model parameters are calculated using backpropagation.\n",
    "4. **Optimizer Step**: The optimizer updates the modelâ€™s weights and biases to reduce the loss.\n",
    "\n",
    "This loop is repeated for multiple **epochs**, where:\n",
    "- **1 epoch** = 1 full pass through the training dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Why is this necessary?\n",
    "\n",
    "- Neural networks start with random weights.\n",
    "- The only way they learn is through **gradually adjusting** those weights to minimize error.\n",
    "- Each pass through the loop helps the model make **better predictions**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Training Loop Components Summary\n",
    "\n",
    "| Step              | Description                                                  |\n",
    "|-------------------|--------------------------------------------------------------|\n",
    "| Forward Pass      | Model predicts output using current weights                  |\n",
    "| Loss Calculation  | Measures how far predictions are from true values            |\n",
    "| Backward Pass     | Computes how to change weights to reduce the loss            |\n",
    "| Optimizer Step    | Applies those changes to update the modelâ€™s parameters       |\n",
    "| Repeat for Epochs | Iterate over the entire dataset multiple times               |\n",
    "\n",
    "---\n",
    "\n",
    "> Without this loop, the neural network cannot improve â€” it would just keep guessing randomly.\n",
    "\n",
    "> No. of iterations = epochs, similar to timesteps when using ABAQUS! the neural network cannot improve â€” it would just keep guessing randomly.\n",
    "\n",
    "##  What is an Epoch in Machine Learning?\n",
    "\n",
    "An **epoch** is **one complete pass** through the **entire training dataset**.\n",
    "\n",
    "Imagine your model as a student studying a textbook (the dataset):\n",
    "- **One epoch** = the student reads the entire book once.\n",
    "- **Multiple epochs** = the student reads the book multiple times to better memorize and understand.\n",
    "\n",
    "---\n",
    "\n",
    "##  Why Do We Need Multiple Epochs?\n",
    "\n",
    "- **First epoch**: The model starts with random guesses and learns basic patterns.\n",
    "- **Subsequent epochs**: The model adjusts its internal weights and improves based on errors from the previous pass.\n",
    "- With each pass, the model *learns from its mistakes* and gets better.\n",
    "\n",
    "---\n",
    "\n",
    "##  Effect of Increasing Epochs\n",
    "\n",
    "- Initially: Training improves rapidly.\n",
    "- Later: Improvements become smaller.\n",
    "- Too many: Risk of **overfitting** â€” the model memorizes training data and fails on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "##  Epochs vs. Batches vs. Iterations\n",
    "\n",
    "| Term       | Meaning                                               |\n",
    "|------------|--------------------------------------------------------|\n",
    "| **Epoch**  | One full pass over the training data                   |\n",
    "| **Batch**  | A subset of the training data used for one update     |\n",
    "| **Iteration** | One update of weights (one batch passed)           |\n",
    "\n",
    "If you have 1000 images and use batch size 100:\n",
    "- 1 epoch = 10 iterations\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "- Increasing epochs lets the model learn **better patterns**.\n",
    "- But you need to stop when loss stops improving (use early stopping or plotting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5239d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 57.9176\n",
      "Epoch 2/40, Loss: 52.9423\n",
      "Epoch 3/40, Loss: 49.9017\n",
      "Epoch 4/40, Loss: 47.7492\n",
      "Epoch 5/40, Loss: 45.9692\n",
      "Epoch 6/40, Loss: 44.3920\n",
      "Epoch 7/40, Loss: 42.9471\n",
      "Epoch 8/40, Loss: 41.5939\n",
      "Epoch 9/40, Loss: 40.2958\n",
      "Epoch 10/40, Loss: 39.0376\n",
      "Epoch 11/40, Loss: 37.7980\n",
      "Epoch 12/40, Loss: 36.5544\n",
      "Epoch 13/40, Loss: 35.2890\n",
      "Epoch 14/40, Loss: 34.0277\n",
      "Epoch 15/40, Loss: 32.7460\n",
      "Epoch 16/40, Loss: 31.4347\n",
      "Epoch 17/40, Loss: 30.1014\n",
      "Epoch 18/40, Loss: 28.7502\n",
      "Epoch 19/40, Loss: 27.3722\n",
      "Epoch 20/40, Loss: 25.9829\n",
      "Epoch 21/40, Loss: 24.5831\n",
      "Epoch 22/40, Loss: 23.2142\n",
      "Epoch 23/40, Loss: 21.8538\n",
      "Epoch 24/40, Loss: 20.5022\n",
      "Epoch 25/40, Loss: 19.1721\n",
      "Epoch 26/40, Loss: 17.8869\n",
      "Epoch 27/40, Loss: 16.6020\n",
      "Epoch 28/40, Loss: 15.2761\n",
      "Epoch 29/40, Loss: 14.0816\n",
      "Epoch 30/40, Loss: 13.3363\n",
      "Epoch 31/40, Loss: 12.6293\n",
      "Epoch 32/40, Loss: 13.1586\n",
      "Epoch 33/40, Loss: 11.3052\n",
      "Epoch 34/40, Loss: 9.0701\n",
      "Epoch 35/40, Loss: 8.2718\n",
      "Epoch 36/40, Loss: 11.9878\n",
      "Epoch 37/40, Loss: 8.8909\n",
      "Epoch 38/40, Loss: 10.9573\n",
      "Epoch 39/40, Loss: 5.9716\n",
      "Epoch 40/40, Loss: 5.0774\n",
      "Predictions: tensor([1, 0, 2, 0, 2, 1, 1, 2, 1, 2])\n",
      "Targets    : tensor([1, 0, 2, 0, 2, 1, 1, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "#Training loop skeleton:\n",
    "#num_epochs = 10 #Number of times we pass through the entire dataset => Loss was around 70-80\n",
    "num_epochs = 40 #Now, let's use more epochs!\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0 #Tracks total loss for a specific epoch\n",
    "\n",
    "    #Now, in an epoch, we iterate over mini-batches of data\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device) #Moved to GPU!\n",
    "\n",
    "            #Now, dog, which is 0 needs to become 0.0, Elephant which is 2->2.0\n",
    "            #Convert to float so that we can add/subtract in BCE\n",
    "            #labels = labels.float().unsqueeze(1)\n",
    "        #But, we are doing CrossEntropyLoss!!!\n",
    "        #So, we remove that line, keep it integer valued\n",
    "\n",
    "        #Clear previous gradients for each epoch, we get a different value\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Forward pass first, using linear initialization, basically,\n",
    "        #We put train_loader, which is a random batch of images into the model\n",
    "        outputs = model(images) #So now, we have our prediction(yhat)\n",
    "        loss = criterion(outputs,labels)\n",
    "        #Now, computation of loss(Classification Cross-Entropy): Takes in the prediction(yhat) and label(y)\n",
    "        #First mistake: this is for one class:  \n",
    "        #For multi-class classification, we need to have a different loss:\n",
    "        \n",
    "        #Now, perform backpropagation, simply with 1 line, gradients calculated:\n",
    "        loss.backward() \n",
    "\n",
    "        #For our optimizer function: weights and biases to be updated\n",
    "        optimizer.step()\n",
    "\n",
    "        #Now, for this batch for this epoch, add the loss onto running loss\n",
    "        running_loss+= loss.item()\n",
    "    \n",
    "    #Now, let's see what the loss of each iteration/epoch is:\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "#Now, printing predictions and targets:\n",
    "images, labels = next(iter(train_loader))\n",
    "outputs = model(images.to(device))\n",
    "_, preds = torch.max(outputs, 1)\n",
    "\n",
    "print(\"Predictions:\", preds[:10])\n",
    "print(\"Targets    :\", labels[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4cad8",
   "metadata": {},
   "source": [
    "### Testing Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30d6fd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[206  68  32]\n",
      " [ 50 170  50]\n",
      " [ 26  68 198]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "model.eval() #Evaluation mode\n",
    "\n",
    "#For collecting all predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images) #Forward Pass\n",
    "        _, predicted = torch.max(outputs.data,1) #Choose class with highest score as prediction\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "#Confusion Matrix:\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(cm)\n",
    "#Plotting:\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690f4db",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "The original goal of this project was to build a basic feedforward neural network to classify animal images (`dog`, `elephant`, `butterfly`) from the Animals-10 dataset. Throughout the development process, several key modifications were made to improve model performance, stability, and training correctness. Below is a complete list of all refinements and theoretical insights added along the way:\n",
    "\n",
    "## 1. Model Architecture Enhancements\n",
    "\n",
    "| Update | Description |\n",
    "|--------|-------------|\n",
    "| **Increased Neurons** | Changed architecture from a minimal 3-2-1 structure to a deeper network with more neurons in each layer, enabling the model to learn more complex patterns. |\n",
    "| **Changed Activation to ReLU** | Replaced `Sigmoid` with `ReLU` in hidden layers for better convergence, especially in deeper networks. |\n",
    "| **Reason** | ReLU avoids the vanishing gradient problem and speeds up training compared to Sigmoid. |\n",
    "| **Final Output Layer** | For multi-class output, the final layer outputs raw logits instead of sigmoid probabilities. |\n",
    "\n",
    "## 2. Loss Function Transition\n",
    "\n",
    "### ðŸ”„ Why We Changed the Loss Function\n",
    "\n",
    "| Aspect                  | Details                                                                 |\n",
    "|-------------------------|-------------------------------------------------------------------------|\n",
    "| **Initial Choice**      | `Binary Cross Entropy (BCE)`                                            |\n",
    "| **Issue**               | BCE is only suitable for **binary classification** tasks.               |\n",
    "|                         | We are solving a **multi-class** problem (e.g., dog, butterfly, elephant). |\n",
    "| **Problem Faced**       | Using BCE led to errors due to incorrect output and target shapes.      |\n",
    "| **Final Choice**        | Switched to `CrossEntropyLoss()` â€” designed for multi-class classification. |\n",
    "| **Result**              | Model now accepts raw logits and properly computes multi-class loss.    |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® Key Differences\n",
    "\n",
    "| Loss Type                  | Formula                                                                 | Notes                                                                 |\n",
    "|---------------------------|-------------------------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| **Binary Cross Entropy**  | `- [ y * log(yÌ‚) + (1 - y) * log(1 - yÌ‚) ]`                             | Expects probabilities (`yÌ‚` in [0, 1]), typically used with `Sigmoid`.|\n",
    "| **Categorical Cross Entropy** | `-log( e^{z_k} / âˆ‘ e^{z_j} )`<br> â†’ `-log(Softmax(logits)[k])`        | Expects raw logits (no softmax needed). Best for multi-class output. |\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… `CrossEntropyLoss()` is appropriate for classification problems where each input belongs to **exactly one class out of many**.\n",
    "\n",
    "\n",
    "## 3. Training Loop and Optimizer\n",
    "\n",
    "| Element | Explanation |\n",
    "|---------|-------------|\n",
    "| **Loss Tracking** | Implemented per-epoch loss reporting to observe convergence over time. |\n",
    "| **Optimizer Used** | `Stochastic Gradient Descent (SGD)` with fixed learning rate. |\n",
    "| **Epoch Intuition** | Explained epochs as \"full passes through the dataset\", allowing the model to iteratively refine weights. |\n",
    "| **Overfitting Risk** | Discussed how increasing epochs too much can lead to overfitting on training data. |\n",
    "\n",
    "## 4. Output Interpretation Fix\n",
    "\n",
    "| Update | Explanation |\n",
    "|--------|-------------|\n",
    "| **Model Output (`yÌ‚`)** | Clarified that logits (not probabilities) should be passed to `CrossEntropyLoss`. |\n",
    "| **Prediction Strategy** | After training, `argmax` was used to convert logits into predicted labels. |\n",
    "\n",
    "## 5. Evaluation Improvements\n",
    "\n",
    "| Topic | Details |\n",
    "|-------|---------|\n",
    "| **Confusion Matrix** | Used to evaluate class-wise performance. |\n",
    "| **Accuracy Metric** | Implemented to check model's overall predictive success. |\n",
    "| **Precision/Recall Discussion** | Discussed their role in imbalanced datasets (though not yet implemented). |\n",
    "\n",
    "## 6. Dataset Management Fixes\n",
    "\n",
    "| Fix | Description |\n",
    "|-----|-------------|\n",
    "| **Unbalanced Labels** | Identified severe class imbalance (e.g., 1446 dog images vs. 300 others). |\n",
    "| **Solution** | Balanced the dataset by sampling an equal number (e.g., 1446) from each class. |\n",
    "| **Manual Verification** | Counted class-wise samples to confirm balance before training. |\n",
    "\n",
    "## Final Remarks\n",
    "\n",
    "This iterative project evolved from a basic neural net to a well-structured, multi-class classification system with correct architectural, loss function, and dataset handling strategies. Each improvement addressed a specific challenge:\n",
    "\n",
    "- Inappropriate loss function â†’ switched to `CrossEntropyLoss`\n",
    "- Vanishing gradients â†’ switched to `ReLU`\n",
    "- Class imbalance â†’ fixed with balanced sampling\n",
    "- Output mismatch â†’ corrected with `argmax` + logits\n",
    "\n",
    "> The process has strengthened both your understanding of neural networks and your practical skills in debugging and refining real-world ML systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
